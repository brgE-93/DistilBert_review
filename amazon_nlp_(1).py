# -*- coding: utf-8 -*-
"""amazon_NLP (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fWzNUTnN8M6slmodOj6bDDQXTt2EJmAy

Business Problem. My task is to build models which can identify the sentiment (from very satisfying to very unsatisfied) of each of these non-rated interactions.
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

import nltk
from nltk import word_tokenize
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from collections import Counter
from wordcloud import WordCloud
import re

"""# Exploratory Data Analysis:EDA"""

#Get our data
Amazon_data=pd.read_csv('dataset_18000.csv')
df=Amazon_data.copy()

def get_len(dataframe):
  return len(dataframe)

print("We have",get_len(df),"data")

def null_values():
  return df.isnull().values.any()

#if True it means we have null value
null_values()

def remove_null_rows(df):
  df=df.dropna()
  print("Number of rows after removing NUll data:",get_len(df))

remove_null_rows(df)

df.head()

#Remove useless column for our sentiment analysis
cl=['ProductId','UserId','ProfileName','HelpfulnessNumerator','HelpfulnessDenominator','Time',]
def new_df(df,cl):
  return df.drop(columns=cl)

df=new_df(df,cl)

df.head()

def wordinreview(dataframe):
    dic={}
    for index,rows in dataframe.iterrows():
        dic[index]=len(rows['Text'].split(" "))
    return pd.Series(dic)

## Getting the number of words by splitting them by a space
def hist_lenght_word(df):
  words_per_review=wordinreview(df)
  words_per_review.hist(bins = 100)
  plt.xlabel('Review Length (words)')
  plt.ylabel('Frequency')
  plt.show()

hist_lenght_word(df)

#in average we have  word per review
def Print_info_word():
    print('Average words:', wordinreview(df).mean())
    print('max num words :', wordinreview(df).max())
    print('Skewness:', wordinreview(df).skew())
Print_info_word()

"""We can see that the number of words per user review is highly positive skewed(4.3) with mean of 79.5, which means that on average, user reviews have 79 words.We can have very long review , until 1772 words ."""

#distribution rating
rating_percent=100*df.Score.value_counts()/len(df)
rating_percent.plot.bar()
plt.show()

"""We can see that the distribution is quite skewed, with a giant number of 5s and very few 3s, 2s, and 1s.

vizualisation with WorldCLoud
"""

from PIL import Image
from nltk.corpus import stopwords
from wordcloud import ImageColorGenerator,STOPWORDS,WordCloud

# Combine all the text into a single string
def Print_Word_Cloud():
    text = ' '.join(df['Text'])



    wordcloud = WordCloud(max_font_size=100, # Maximum font size for the largest word
                          max_words=100, # The maximum number of words
                          background_color="white", # Background color for the word cloud image
                          scale = 10, # Scaling between computation and drawing
                          width=800, # Width of the canvas
                          height=400 # Height of the canvas
                         ).generate(text)

    plt.figure()
    plt.imshow(wordcloud,
               interpolation="bilinear") # to make the displayed image appear more smoothly
    plt.axis("off")
    plt.show()

Print_Word_Cloud()

"""Br its for the space its not relevant.taste, love , amazon (the name of the website) love and great (thats why we have more 5 rating

### Check for pre-processing
"""

#remove links and tags
def Clean_reviews(text):
    # Remove links
    text = re.sub(r'http\S+|www\S+|https\S+', '', text)

    # Remove tags
    text = re.sub(r'<.*?>', '', text)

    return text

# Apply the function to the 'Text' column
df['Text'] = df['Text'].apply(Clean_reviews)

Print_Word_Cloud()
Print_info_word()

!pip install emoji

# Function to check if emojis exist in text
def has_emoji(text):
    for character in text:
        if emoji.is_emoji(character):
            return True
    return False

# Find rows with emojis
rows_with_emoji = df[df['Text'].apply(has_emoji)]

# Print rows and their indices
for index, row in rows_with_emoji.iterrows():
    print("Index:", index)
    print("Text:", row['Text'])
    print()

# Count the number of rows with emojis
num_rows_with_emoji = len(rows_with_emoji)

print("Number of rows with emojis:", num_rows_with_emoji)

"""The "emoji" here are the mini logo beside the name of some manifacturies so we leave them as it is ."""

!pip install --upgrade scikit-learn

# Separate the text and score columns
text = df['Text']
score = df['Score']

# Define the range of n-grams
# Define the range of n-grams you want to consider
ngram_range = (2, 3)  # Consider bi-grams and tri-grams

# Create CountVectorizer with n-gram range and exclude stopwords
vectorizer = CountVectorizer(ngram_range=ngram_range, stop_words='english', min_df=1)

# Fit the vectorizer on the text data
X = vectorizer.fit_transform(text)

# Get the n-gram features
ngram_features = vectorizer.get_feature_names_out()

# Calculate the n-gram frequencies
ngram_counts = X.sum(axis=0)

# Create a DataFrame to store the n-gram frequencies
ngram_df = pd.DataFrame(ngram_counts, columns=ngram_features)
ngram_df = ngram_df.transpose()
ngram_df.columns = ['Frequency']

# Sort the n-grams by frequency in descending order
ngram_df = ngram_df.sort_values('Frequency', ascending=False)

# Print the top n-grams and their frequencies
num_top_ngrams = 10  # Number of top n-grams to display
print(ngram_df.head(num_top_ngrams))

"""Those are the most  frequented bi-gram, and we see we have few positive sentence.

# VADER Sentiment Scores
"""

from nltk.sentiment import SentimentIntensityAnalyzer
from tqdm.notebook import tqdm

"""I didn't touch punctuations and case for now as VADER SENTIMENT ANALYSIS SCORES are affected by factors like punctuations,capitalization,preceeding-trigrams,degree modifiers,conjunctions etc"""

nltk.download('vader_lexicon')

analyser_object=SentimentIntensityAnalyzer()

def vader_scores():
    dic= {}
    for i, row in tqdm(df.iterrows(), total=len(df)):
        review=row['Text']
        myid=row['Id']
        dic[myid]=analyser_object.polarity_scores(review)
    return dic

"""Vader retuns 4 values:


*   a neutrality score



*   a positivity score


*   a negativity score
*   an overall score that summarizes the previous scores




"""

vader_scores()

#See on an example
print(df.Text[1],"   Vader score : ",analyser_object.polarity_scores(df.Text[1]))

"""If compound is superior to 0.05 its a positive review in general and if its inferior to -0.05 its negative in general ."""

vaders_df=pd.DataFrame(vader_scores()).T
vaders_df=vaders_df.reset_index().rename(columns={'index': 'Id'})
vaders_df=vaders_df.merge(df,how='left')

vaders_df

#Plot for vader sentiment
fig, axs = plt.subplots(1, 4, figsize=(15, 4))
sns.barplot(data=vaders_df, x='Score', y='pos', ax=axs[0])
sns.barplot(data=vaders_df, x='Score', y='neu', ax=axs[1])
sns.barplot(data=vaders_df, x='Score', y='neg', ax=axs[2])
sns.barplot(data=vaders_df,x='Score',y='compound',ax=axs[3])
axs[0].set_title('Positive')
axs[1].set_title('Neutral')
axs[2].set_title('Negative')
axs[3].set_title('Compound')
plt.tight_layout()
plt.show()

# Calculate the sentiment distribution
sentiment_distribution = vaders_df['compound'].value_counts()
plt.bar(sentiment_distribution.index, sentiment_distribution.values)
plt.xlabel('Sentiment Score')
plt.ylabel('Count')
plt.title('Sentiment Distribution')
plt.show()

"""We can observe that most of the reviews tend to be neutral or being positive ( here we dont know the degree of the positivity)"""

# 2.5 Check the correlation between sentiment score (compound) and rating
import numpy as np
import scipy.stats as stats
print("The correlation coefficient between sentiment score (compound) and rating is {0[0]: .4f} with a p-value of {0[1]: .4f}.".format(stats.pearsonr(vaders_df['Score'], vaders_df["compound"])))
vaders_df.groupby("Score").mean()["compound"].plot(kind="bar", figsize=(10, 6))
plt.title("Avg. Sentiment Score (Compound) per Rating")
plt.show()

"""There is a moderate positive correlation between the sentiment score and the rating. Higher sentiment scores tend to be associated with higher ratings, and this relationship is statistically significant.The VADER model proves to be a good powerful tool for sentiment classification.But with pretrained model we will have more accurate classification.

---

# **Le modele DistilBert**

---
"""

!pip install transformers

import pandas as pd
import torch
import torch.nn as nn
from torch.optim.lr_scheduler import ReduceLROnPlateau
from sklearn.model_selection import train_test_split
from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, AdamW
from torch.utils.data import TensorDataset, DataLoader
from tqdm import tqdm
from imblearn.over_sampling import RandomOverSampler
from nltk.corpus import stopwords
import pickle
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,confusion_matrix, classification_report

#torch.cuda.empty_cache()

nltk.download('stopwords')

"""### splitting ,dataset,dataloader tensors"""

X= df['Text']
y=df['Score']
# Split the dataset into train, validation, and test sets
train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)
train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)

X_train = train_data['Text'].values
y_train = train_data['Score'].values

X_val = val_data['Text'].values
y_val = val_data['Score'].values

X_test = test_data['Text'].values
y_test = test_data['Score'].values

pd.DataFrame(y_train).value_counts()

# Initialize the tokenizer and DistilBERT
stop_words = set(stopwords.words('english'))
num_classes = len(df['Score'].unique())
tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased', do_lower_case=True, stop_words=stop_words)
#model architecture
model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=num_classes)
#print(model)

# Tokenization
train_encodings = tokenizer.batch_encode_plus(X_train.tolist(), padding=True, truncation=True, max_length=512, return_tensors='pt')
val_encodings = tokenizer.batch_encode_plus(X_val.tolist(), padding=True, truncation=True, max_length=512, return_tensors='pt')
test_encodings = tokenizer.batch_encode_plus(X_test.tolist(), padding=True, truncation=True, max_length=512, return_tensors='pt')

# Conversion in  pytorch tensors
#Dataset
#for tensors rating goes from 0 to 4
train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], torch.tensor(y_train - 1))
val_dataset = TensorDataset(val_encodings['input_ids'], val_encodings['attention_mask'], torch.tensor((y_val - 1)))
test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], torch.tensor((y_test - 1)))

#Optimization in  mini-batch
#Dataloaders
batch_size = 16
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size)
test_loader = DataLoader(test_dataset, batch_size=batch_size)

"""The reason such a few epochs
were applied in our model analysis was that we utilized the fine-tuning feature of the model; the model was welltrained, and only a few epochs were necessary for tuning.
The number of epochs can be increased, however, it will give rise to overfitting problems as well as take more time for the model to train and with google collab its not fast.

### Fine-tuning and optimization of the model
"""

#regularization with dropout
# optimizer and loss function
optimizer = AdamW(model.parameters(), lr=2e-5)
loss_function = torch.nn.CrossEntropyLoss()

#torch.autograd.set_detect_anomaly(True)
model.dropout = nn.Dropout(0.5)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# loop
train_f1_scores = []
train_accuracies = []
val_f1_scores = []
val_accuracies = []
train_losses = []
val_losses = []

scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=2, verbose=True)
best_val_f1 = 0.0
early_stopping_counter = 0


for epoch in tqdm(range(2), desc="Epochs"):
    model.train()
    total_loss = 0
    train_predictions = []
    train_labels = []

    #for batch in tqdm(train_loader, desc="Batches", leave=False):
    for batch in train_loader:
        input_ids, attention_mask, labels = batch

        optimizer.zero_grad()


        outputs = model(
            input_ids = input_ids.to(device),
            attention_mask = attention_mask.to(device),
            labels = labels.to(device)
        )

        # loss function
        loss = outputs.loss
        total_loss += loss.item()
        # backward and update weigh
        loss.backward()
        optimizer.step()


        train_predictions.extend(torch.argmax(outputs.logits, dim=1).tolist())
        train_labels.extend(labels.tolist())

    # Calculate metrics for the training set

    train_f1 = f1_score(train_labels, train_predictions, average='micro')
    train_accuracy = accuracy_score(train_labels, train_predictions)
    train_f1_scores.append(train_f1)
    train_accuracies.append(train_accuracy)
    print(f"Epoch {epoch+1} - Training F1-score: {train_f1:.4f}, Accuracy: {train_accuracy:.4f}")
    avg_loss = total_loss / len(train_loader)
    train_losses.append(avg_loss)
    print(f"Époque {epoch+1} - Perte d'entraînement : {avg_loss}")


    model.eval()
    with torch.no_grad():
      predicted_labels = []
      true_labels = []
      val_loss = 0

      for batch in val_loader:
          input_ids, attention_mask, labels = batch
          outputs = model(
              input_ids = input_ids.to(device),
              attention_mask = attention_mask.to(device),
              labels=labels.to(device)
          )

          predicted_labels.extend(torch.argmax(outputs.logits, dim=1).tolist())
          true_labels.extend(labels.tolist())
          val_loss += outputs.loss.item()

      avg_val_loss = val_loss / len(val_loader)
      val_losses.append(avg_val_loss)
    print(f"Epoch {epoch+1} - Validation Loss: {avg_val_loss:.4f}")

    #Micro-average  when there are class imbalances
    accuracy = accuracy_score(true_labels, predicted_labels)
    f1 = f1_score(true_labels, predicted_labels, average='micro')
    val_f1_scores.append(f1)
    val_accuracies.append(accuracy)


    print(f"Accuracy: {accuracy:.4f}")
    print(f"F1-score: {f1:.4f}")

    # Check if validation F1-score has improved
    if f1 > best_val_f1:
        best_val_f1 = f1
        early_stopping_counter = 0
        torch.save(model.state_dict(), 'best_model.pt')  # Save the best model
    else:
        early_stopping_counter += 1
        if early_stopping_counter >= 3:  # Stop early if validation F1-score doesn't improve for a certain number of consecutive epochs
            print("Early stopping triggered.")
            break

    # Adjust the learning rate based on validation F1-score
    scheduler.step(f1)

"""We try different numbers for the hyperparameters and epochs and those ones left seems to get the best result and avoiding overfitting.."""

model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=5)
model.load_state_dict(torch.load('best_model.pt'))

"""### Final evaluation  on the  unseen data"""

# Evaluation on the test set
model.to(device)
model.eval()
test_predictions = []
test_labels = []

with torch.no_grad():
    for batch in test_loader:
        input_ids, attention_mask, labels = batch
        outputs = model(input_ids=input_ids.to(device), attention_mask=attention_mask.to(device),labels = labels.to(device))
        logits = outputs.logits
        predictions = torch.argmax(logits, dim=1)

        test_predictions.extend(predictions.tolist())
        test_labels.extend(labels.tolist())
    accuracy_score(test_labels, test_predictions)
    precision = precision_score(test_labels, test_predictions, average='micro')
    recall = recall_score(test_labels, test_predictions, average='micro')
    f1 = f1_score(test_labels, test_predictions, average='micro')


# Confusion matrix
confusion_mat = confusion_matrix(test_labels, test_predictions)
classification_rep = classification_report(test_labels, test_predictions)

print("Confusion Matrix:")
print(confusion_mat)
#print("Classification Report:")
#print(classification_rep)

print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-score: {f1:.4f}")

"""Our DISTILBERT model had an accuracy of 0.7493 and a F1-score of 0.7372 in predicting the testing group’s score based
on the text. These results are very impressive given that the reviews were subjective and written by individuals.

###  Prediction on unseen review
"""

# Save the trained model

model.eval()

# Function to predict the rating of a new text input
def predict_rating(text):
    encoding = tokenizer.encode_plus(text, padding=True, truncation=True, max_length=128, return_tensors='pt')
    input_ids = encoding['input_ids']
    attention_mask = encoding['attention_mask']

    with torch.no_grad():
        outputs = model(input_ids=input_ids.to(device), attention_mask=attention_mask.to(device))
        logits = outputs.logits
        predicted_rating = torch.argmax(logits, dim=1)

    return predicted_rating.item()

# Example usage of the predict_rating function
new_text = "Great price on a great gluten-free product!,We have paid over $5 a package in health food stores for this mix.  Buying via Amazon is both cheaper and easier thanks to the direct to the door shipping!  The mix works great in our bread machine and comes out perfect every time!  Our kids love it and usually eat half a loaf as soon as its done!"

predicted_rating = predict_rating(new_text)
if predicted_rating ==4:
  print( "Really positive review")
if predicted_rating ==3:
  print( "positive review")
if predicted_rating ==2:
  print( "neutral review")
if predicted_rating ==1:
  print( "bad review ")
if predicted_rating ==0:
  print( "Really bad review")

"""### Pickle our model for futur use"""

# Pickle the model
with open('best_model.pkl', 'wb') as f:
    pickle.dump(model, f)

# Load the pickled model
with open('best_model.pkl', 'rb') as f:
   model = pickle.load(f)